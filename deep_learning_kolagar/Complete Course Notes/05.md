
---

# جزوه آموزشی: توابع هزینه (Loss Functions) در شبکه‌های عصبی
**موضوع:** محاسبه خطا در مسائل رگرسیون و کلاسیفیکیشن (با تمرکز بر کراس آنتروپی)

## ۱. مقدمه
در شبکه‌های عصبی، هدف اصلی آموزش مدل، کاهش اختلاف بین "خروجی پیش‌بینی شده" (Predicted) و "مقدار واقعی" (True) است. برای این کار باید مقدار خطا (Error) محاسبه شود. نوع محاسبه خطا بستگی به نوع مسئله (رگرسیون یا کلاسیفیکیشن) دارد.

---

## ۲. محاسبه خطا در مسائل رگرسیون (Regression)

در مسائل رگرسیون، ما به دنبال پیش‌بینی یک مقدار عددی پیوسته هستیم.

* **ساختار لایه آخر (Output Layer):** معمولاً دارای **یک نورون** است، زیرا یک مقدار مشخص را پیش‌بینی می‌کند (مثلاً قیمت خانه).
* **خروجی:** شبکه به ازای هر نمونه (Sample)، یک مقدار $y_{predict}$ تولید می‌کند.
* **روش محاسبه:** مقدار پیش‌بینی شده ($y_{predict}$) با مقدار واقعی ($y_{true}$) مقایسه می‌شود.

### توابع هزینه رایج در رگرسیون:
برای محاسبه فاصله بین مقدار واقعی و پیش‌بینی شده از روش‌های زیر استفاده می‌شود:
1.  **MSE (Mean Squared Error):** یا خطای میانگین مربعات (نرم L2).
2.  **MAE (Mean Absolute Error):** یا خطای میانگین قدر مطلق (نرم L1).

<img width="1095" height="545" alt="image" src="https://github.com/user-attachments/assets/48e898ab-6245-4919-97e3-ceb2808a0064" />

---

## ۳. محاسبه خطا در مسائل کلاسیفیکیشن (Classification)

در اینجا هدف دسته‌بندی داده‌ها به کلاس‌های مختلف است.

### ساختار شبکه:
* **لایه ورودی (Input Layer):** داده‌ها وارد می‌شوند.
* **لایه‌های پنهان (Hidden Layers):** پردازش انجام می‌شود.
* **لایه خروجی (Output Layer):** تعداد نورون‌های این لایه باید دقیقاً **برابر با تعداد کلاس‌ها** باشد.

### مثال عملی ویدیو:
فرض کنید مسئله‌ای با **۳ کلاس** داریم:
1.  اسب (Horse)
2.  سگ (Dog)
3.  گربه (Cat)

بنابراین لایه خروجی ۳ نورون دارد. معمولاً از یک **تابع فعال‌ساز (Activation Function)** (مانند Softmax) در لایه آخر استفاده می‌شود تا خروجی‌ها را به **احتمالات** تبدیل کند.

### تعریف مقادیر (مثال عددی):
فرض کنید برای یک تصویر خاص (که تصویر "سگ" است)، شبکه مقادیر احتمالی زیر را پیش‌بینی کرده است:

**الف) مقادیر پیش‌بینی شده ($y_{predict}$):**
* کلاس اول (اسب): $0.4$ (۴۰٪)
* کلاس دوم (سگ): $0.4$ (۴۰٪)
* کلاس سوم (گربه): $0.2$ (۲۰٪)

**ب) مقادیر واقعی ($y_{true}$) با روش One-Hot Encoding:**
در این روش، کلاسی که صحیح است مقدار **۱** و بقیه مقدار **۰** می‌گیرند. چون تصویر "سگ" (کلاس دوم) است:
* بردار واقعی: $[0, 1, 0]$
<img width="1117" height="461" alt="image" src="https://github.com/user-attachments/assets/0f1a27e7-bff8-4c28-bd1e-fe0b3e7dcafe" />

---

## ۴. تابع هزینه کراس آنتروپی (Cross-Entropy Loss)

در مسائل کلاسیفیکیشن، برای محاسبه خطا از فرمول کراس آنتروپی استفاده می‌شود.

### فرمول ریاضی:
$$CrossEntropy = - \sum (y_{true, i} \times \ln(y_{predict, i}))$$

* $y_{true}$: مقدار واقعی (که یا ۰ است یا ۱).
* $\ln(y_{predict})$: لگاریتم طبیعی مقدار پیش‌بینی شده.
* $\Sigma$: مجموع حاصل‌ضرب‌ها برای تمام کلاس‌ها.

### محاسبه دستی خطا (بر اساس مثال بالا):
می‌خواهیم ببینیم با پیش‌بینی‌های $[0.4, 0.4, 0.2]$ و مقدار واقعی $[0, 1, 0]$ خطا چقدر است:

$$Loss = - \left[ (0 \times \ln(0.4)) + (1 \times \ln(0.4)) + (0 \times \ln(0.2)) \right]$$

**تحلیل:**
1.  ترم‌های مربوط به کلاس‌های نادرست (اسب و گربه) چون در $y_{true}$ صفر هستند، حذف می‌شوند ($0 \times \dots = 0$).
2.  فقط ترم مربوط به کلاس صحیح (سگ) باقی می‌ماند.

$$Loss = - (1 \times \ln(0.4))$$

با محاسبه مقدار لگاریتم:
$$Loss \approx - (-0.916) = 0.916$$
*(در ویدیو مقدار تقریبی ۰.۹۶ ذکر شد)*
<img width="1084" height="198" alt="image" src="https://github.com/user-attachments/assets/c524e763-b861-4f1e-8713-710c31a71779" />
<img width="1022" height="229" alt="image" src="https://github.com/user-attachments/assets/6393896a-04c9-4394-8480-1f53fbf0d0a3" />

**نتیجه:** این عدد (نزدیک به ۱) نشان‌دهنده **خطای بالا** است و وضعیت مدل خوب نیست، زیرا مدل با اطمینان کمی (۴۰٪) کلاس درست را حدس زده است.

---

## ۵. حالت ایده‌آل (خطای صفر)

چه زمانی خطا بسیار کم یا صفر می‌شود؟ زمانی که مدل با اطمینان کامل (۱۰۰٪) کلاس درست را پیش‌بینی کند.

**فرض کنید پیش‌بینی مدل تغییر کند به:**
* پیش‌بینی ($y_{predict}$): $[0, 1, 0]$ (یعنی ۱۰۰٪ مطمئن است که عکس سگ است).
* واقعیت ($y_{true}$): $[0, 1, 0]$

### محاسبه مجدد خطا:
$$Loss = - (1 \times \ln(1))$$

می‌دانیم که $\ln(1) = 0$ است. پس:
$$Loss = 0$$

### جمع‌بندی نهایی:
* در کراس آنتروپی، هدف ما این است که احتمال پیش‌بینی شده برای کلاس صحیح به عدد **۱** نزدیک شود.
* هرچقدر پیش‌بینی به ۱ نزدیک‌تر باشد، مقدار $\ln$ به ۰ نزدیک‌تر شده و خطا (Loss) کاهش می‌یابد.
* فقط کلاسی که مقدار واقعی آن ۱ است (Target Class) در محاسبه نهایی خطا تاثیر دارد و بقیه کلاس‌ها صفر می‌شوند.
