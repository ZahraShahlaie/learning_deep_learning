ุจุณุงุฑ ุนุงู! ุฏุฑ ุงุฏุงููุ ุฎูุงุตู ฺฉุงูู ู ุฌุฒุฆุงุช ูุจุงุญุซ ฺฉู ุฏุฑ ูุฏู ูุทุฑุญ ุดุฏ ุฑุง ุจุง ุณุงุฎุชุงุฑุฏู ููุงุณุจุ ุนูุง ูุทุงุจู ุจุง ุฑููุฏ ุชูุถุญุงุช ุดูุงุ ุงุฑุงุฆู ูโุฏูู.

---

# ๐ง ุจูููโุณุงุฒูุง (Optimizers) ุฏุฑ ุดุจฺฉูโูุง ุนุตุจ

**ุชุนุฑู:** ฺฉ ุจูููโุณุงุฒ ุฏุฑ ุดุจฺฉูโ ุนุตุจุ ุงูฺฏูุฑุชู ุงุณุช ฺฉู ุจุฑุง **ุชูุธู ฺฉุฑุฏู ุง ุจูุจูุฏ ูุฒูโูุง ู ุจุงุงุณโูุง** ุดุจฺฉู ุจูโููุธูุฑ ฺฉุงูุด ุชุงุจุน ุฒุงู (Loss Function) ุงุณุชูุงุฏู ูโุดูุฏ.

ุงู ุชุนุฑู ุดูุง ุฑุง ุจู ุงุฏ **ฺฏุฑุงุฏุงู ุฏุณูุช (Gradient Descent)** ูโุงูุฏุงุฒุฏ ฺฉู ููุด ุงุตู ุงูพุชูุงุฒุฑ ุฑุง ุฏุฑ ุงูฺฏูุฑุชู ุงุฏฺฏุฑ ุจุงุฒ ูโฺฉูุฏ.

## ฑ. ฺฏุฑุงุฏุงู ุฏุณูุช (Gradient Descent - GD)

### ฑ.ฑ. ุถุนูโูุง ุงุณุงุณ ฺฏุฑุงุฏุงู ุฏุณูุช
1.  **ุงุณุชูุงุฏู ุงุฒ ฺฉู ุฏุชุงุณุช:** GD ฺฉู ุฏุชุงุณุช ุฑุง ฺฉุฌุง ูฺฏุงู ูโฺฉูุฏุ ฺฏุฑุงุฏุงู ุฑุง ูุญุงุณุจู ูโฺฉูุฏ ู ุณูพุณ ูุฒูโูุง ุฑุง ุขูพุฏุช ูโฺฉูุฏ.
    * **ูุญุฏูุฏุช ุญุงูุธู:** ุจุฑุง ุฏุชุงุณุชโูุง ุจุฒุฑฺฏุ ูุญุฏูุฏุช ูููุฑ ุฌุฏ ุงุฌุงุฏ ูโฺฉูุฏ.
    * **ฺฉูุฏ ุขูพุฏุช:** ุจุฑุง ุขูพุฏุช ูุฒูโูุง ุจุงุฏ ููุชุธุฑ ุจูุงูู ุชุง ฺฉู ุฏุชุงุณุช ุฏุฏู ุดูุฏ.

## ฒ. ุงุณุชูฺฉุงุณุชฺฉ ฺฏุฑุงุฏุงู ุฏุณูุช (Stochastic Gradient Descent - SGD)

ุจุฑุง ุญู ุถุนูโูุง GDุ ุงุฏู **Mini-Batch** ูุทุฑุญ ุดุฏ.

* **SGD ฺุณุชุ** ููุงู ฺฏุฑุงุฏุงู ุฏุณูุช ุงุณุชุ ุจุง ุงู ุชูุงูุช ฺฉู ุจูโุฌุง ุงุณุชูุงุฏู ุงุฒ ฺฉู ุฏุชุงุณุชุ ุงุฒ **ููโุจฺโูุง** ุงุณุชูุงุฏู ูโฺฉูุฏ. ุฏุฑ ูุฑ ุชฺฉุฑุงุฑุ ฺฉ ููโุจฺ ุจุฑุฏุงุดุชู ูโุดูุฏ ู ุดุจฺฉู ุจุง ุขู ุขููุฒุด ุฏุงุฏู ูโุดูุฏ.

### ฒ.ฑ. ูุดฺฉูุงุช ููโุจฺโูุง
* **ุนุฏู ุชูุงุฒู ฺฉูุงุณโูุง:** ุงฺฏุฑ ููโุจฺโูุง ุจูโุทูุฑ ุชุตุงุฏู ุดุงูู ุชุนุฏุงุฏ ูุงูุชูุงุฒู ุงุฒ ูููููโูุง ฺฉูุงุณโูุง ูุฎุชูู ุจุงุดูุฏ (ูุซูุงู MNIST: ุงุนุฏุงุฏ ฐ ุชุง น)ุ ุดุจฺฉู ููฺฉู ุงุณุช ุฑู ฺฉูุงุณ ุบุงูุจ ุจุดุชุฑ ุชูุฑฺฉุฒ ฺฉูุฏ ู ุฏฺุงุฑ **ุณูฺฏุฑ (Bias)** ุดูุฏ.
* **ุฑุงูโุญู:** ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุณูฺฏุฑุ ุจูุชุฑ ุงุณุช ููโุจฺโูุง **ุจูโุทูุฑ ุชุตุงุฏู (Stochastic)** ุงุฒ ุฏุชุงุณุช ุงูุชุฎุงุจ ุดููุฏ.

### ฒ.ฒ. ูุดฺฉู ุงุณุงุณ ูุดุชุฑฺฉ (GD ู SGD)
* ูุฑ ุฏู ุงูฺฏูุฑุชู ููฺูุงู ุจุง ูุดฺฉู ฺฏุฑ ฺฉุฑุฏู ุฏุฑ **ูููู ูุญู (Local Minimum)** ููุงุฌู ูุณุชูุฏ ู ููโุชูุงููุฏ ุจู ููุทู ุจููู ุฌูุงู (Global Minimum) ุฏุณุช ุงุจูุฏ.

---

## ณ. ููููุชูู (Momentum)

ุจุฑุง ุญู ูุดฺฉู ฺฏุฑ ฺฉุฑุฏู ุฏุฑ ููููโูุง ูุญูุ ุงุฏู **ููููุชูู (ุชฺฉุงูู)** ูุทุฑุญ ุดุฏ.

* **ุงุฏู:** ูุฑุถ ฺฉูุฏ ุชููพ ุงุฒ ฺฉ ุณุทุญ ุฑูุง ูโุดูุฏ. ุชููพ ฺฉู ุจู ฺฉ ูุฑูุฑูุชฺฏ ูุญู ูโุฑุณุฏุ ุจู ุฏูู ุณุฑุนุช ู ูุณุฑ ฺฉู ุท ฺฉุฑุฏู ุงุณุช (ุชฺฉุงูู)ุ ูโุชูุงูุฏ ุงุฒ ุขู ุนุจูุฑ ฺฉุฑุฏู ู ุจู ุณูุช ููููโูุง ุนููโุชุฑ ุญุฑฺฉุช ฺฉูุฏ.

### ณ.ฑ. ูพุงุฏูโุณุงุฒ ููููุชูู
ุจุฑุง ูพุงุฏูโุณุงุฒ ุงู ุงุฏูุ ฺฉ **ุญุงูุธู** ุฏุฑ ฺฏุฑุงุฏุงู ุงุฌุงุฏ ูโฺฉูู ุชุง **ฺฏุฑุงุฏุงูโูุง ูุจู** ุฑุง ุฏุฑ ูุญุงุณุจุงุช ูุญุงุธ ฺฉูุฏ.

* **ูุฑููู ููููุชูู (ุณูุชุฒ ุดุฏู ุฏุฑ ูุฏู):**
    ุจุฑุง ุขูพุฏุช ูุฒู ($W$):
    $$W_{i+1} = W_i - \eta \cdot (\frac{\partial L}{\partial W_i} + \alpha \cdot \frac{\partial L}{\partial W_{i-1}})$$
    * **ูฺฉุชู ุชูุถุญ ุดูุง:** ุฏุฑ ูุงูุนุ ุจูโุฌุง ุงูฺฉู ููุท ฺฏุฑุงุฏุงู ูุญุธูโุง ุฑุง ูุญุงุธ ฺฉููุ ุนุงูู ุญุฑฺฉุช ูุจู ุฑุง ูุฒ ุฏุฑ ูุธุฑ ูโฺฏุฑู. (ุงู ฺฉุงุฑ ุจุง ูุญุงุณุจู ุณุฑุนุช $V$ ุฏุฑ ูุฑููู ุงุณุชุงูุฏุงุฑุฏ ููููุชูู ุงูุฌุงู ูโุดูุฏ).
* **$\alpha$ (ุถุฑุจ ุขููุง):** ฺฉ ูุงูพุฑูพุงุฑุงูุชุฑ ุงุณุช ฺฉู ุชุนู ูโฺฉูุฏ **ููุฏุงุฑ ุฐุฎุฑู ุชุงุฑุฎฺู (ุญุงูุธู)** ฺูุฏุฑ ุจุงุดุฏ.
    * ุงฺฏุฑ $\alpha = 0$ุ ุจู ููุงู ุงูฺฏูุฑุชู SGD ุจุฑูโฺฏุฑุฏู.
* **ูุฒุช:** ููููุชูู ุจู ูุง ฺฉูฺฉ ูโฺฉูุฏ ฺฉู ุงุฒ ููุงุทู ููฺฉุงู ุนุจูุฑ ฺฉุฑุฏู ู ุจู ููุทู ฺฉููู ุจูุชุฑ ุฏุณุช ูพุฏุง ฺฉูู.

---

## ด. ูุดฺฉู ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate - $\eta$)

ฺฉ ุงุฒ ูุณุงุฆู ููู ุฏุฑ ุจูููโุณุงุฒุ ุชุนู ููุฏุงุฑ **ูุฑุฎ ุงุฏฺฏุฑ ($\eta$)** ุงุณุช.

* **ูุงููู ฺฉู:** ูุฑุฎ ุงุฏฺฏุฑ ุจุงุฏ "Small Enough and Big Enough" ุจุงุดุฏ.
    * **ุงฺฏุฑ $\eta$ ุฎู ฺฉู ุจุงุดุฏ:** ุขููุฒุด ุจุง ุดุจ ุจุณุงุฑ ฺฉู ูพุด ูโุฑูุฏ ู ุฑุณุฏู ุจู ููุทู ุจููู ุฒูุงู ุจุณุงุฑ ุฒุงุฏ (ุงูพุงฺฉโูุง ุจุงูุง) ูโุจุฑุฏ.
    * **ุงฺฏุฑ $\eta$ ุฎู ุฒุงุฏ ุจุงุดุฏ:** ุดุจฺฉู ุงุตูุงู ููโุชูุงูุฏ ุขููุฒุด ุจุจูุฏุ ููุฏุงุฑ ุฒุงู (Loss) ุงูุฒุงุด ูพุฏุง ูโฺฉูุฏ ู ุจุง ูพุฑุดโูุง ุจุฒุฑฺฏ ุงุฒ ุงูพุชููู ุฏูุฑ ูโุดูุฏ.
    * **ุงฺฏุฑ $\eta$ ุจุงูุง ุจุงุดุฏ (ุงูุง ูู ุฎู ุฒุงุฏ):** ุฏุฑ ุงุจุชุฏุง ุฌูุด ุณุฑุน ู ุฎูุจ ุฏุฑ ุงุฏฺฏุฑ ุฏุงุฑู (ุจูโุณุฑุนุช ุจู ูุฒุฏฺฉ ุงูพุชููู ูโุฑุณู)ุ ุงูุง ูพุณ ุงุฒ ุขู ุจู ุฏูู ุฌูุดโูุง ุจุฒุฑฺฏุ ุงุทุฑุงู ููุทู ุงูพุชููู ููุณุงู ูโฺฉูุฏ ู ููโุชูุงูุฏ ุจู ุขู ููฺฏุฑุง ุดูุฏ.

**ุญุงูุช ุจููู (Learning Rate Scheduling):**
* ุฏุฑ ุงุจุชุฏุงุ $\eta$ ุจุงุฏ **ุฒุงุฏ** ุจุงุดุฏ (ุจุฑุง ุญุฑฺฉุช ุณุฑุน ุจู ุณูุช ุงูพุชููู).
* ุณูพุณุ $\eta$ ุจุงุฏ **ฺฉุงูุด** ุงุจุฏ (ุจุฑุง ฺฏุงูโูุง ฺฉูฺฺฉ ู ููฺฏุฑุง ุฏูู ุจู ุงูพุชููู).
* **ูุฏู:** ุฏุณุชุงุจ ุจู ุฏูุช ุจูุชุฑ ุจุง ฺฉูุชุฑู ูุฒุงู ูุตุฑู ููุงุจุน ุณุฎุชโุงูุฒุงุฑ ู ุฒูุงู (ุงูุชุตุงุฏ ฺฉุฑุฏู ูุฑุขูุฏ ุขููุฒุด).

### ด.ฑ. ุฑุงูโุญูโูุง ูุฑุฎ ุงุฏฺฏุฑ ุฏุงูุงูฺฉ

#### ุงูู. ุจุฑูุงููโุฑุฒ ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate Schedule)
* **ฺฉุงูุด ููุง (Exponential Decay):** ุชุบุฑ ูุฑุฎ ุงุฏฺฏุฑ ุจูโุตูุฑุช ุฏุณุช ุจุง ฺฉ ุฑุงุจุทู ุงุฒ ูพุด ุชุนู ุดุฏู.
    $$\eta_i = \eta_0 \cdot e^{-k \cdot i}$$
    * $\eta_i$: ูุฑุฎ ุงุฏฺฏุฑ ุฏุฑ ุงูพูฺฉ $i$.
    * $k$: ูุงูพุฑูพุงุฑุงูุชุฑ ฺฉู ุดุฏุช ฺฉุงูุด ุฑุง ุชุนู ูโฺฉูุฏ.

#### ุจ. ูุฑุฎ ุงุฏฺฏุฑ ุชุทุจู (Adaptive Learning Rate)

ุงู ุฑูุดโูุง ุจูโุตูุฑุช ููุดููุฏ ู ุจุฑ ุงุณุงุณ ฺฏุฑุงุฏุงูุ ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ุจูโุทูุฑ ุฏุงูุงูฺฉ ุชูุธู ูโฺฉููุฏ:

**ุขุฏุงฺฏุฑุงุฏ (AdaGrad)**

* **ุงุฏู:** ฺฏุฑุงุฏุงู ุจู ุตูุฑุช ูุณุชูู ุฏุฑ ุชุนู ูุฑุฎ ุงุฏฺฏุฑ ุงุซุฑ ูโฺฏุฐุงุฑุฏ. ุงฺฏุฑ ฺฏุฑุงุฏุงู ุฒุงุฏ ุจุงุดุฏุ $\eta$ ุจุงูุง ูฺฏู ุฏุงุดุชู ูโุดูุฏุ ุงฺฏุฑ ฺฏุฑุงุฏุงู ฺฉู ุดูุฏ (ูุฒุฏฺฉ ุดุฏู ุจู ุงูพุชููู)ุ $\eta$ ูพุงู ุขูุฑุฏู ูโุดูุฏ.
* **ุฑุงุจุทู ุงูุจุงุดุช ฺฏุฑุงุฏุงู ($G_i$):**
    $$G_i = G_{i-1} + \left( \frac{\partial L}{\partial W_i} \right)^2$$
* **ุฑุงุจุทู ุจูโุฑูุฒุฑุณุงู ูุฒู ($W_{i+1}$):**
    $$W_{i+1} = W_i - \frac{\eta}{\sqrt{G_i + \epsilon}} \cdot \frac{\partial L}{\partial W_i}$$
    * **$\epsilon$ (ุงูพุณููู):** ููุฏุงุฑ ุจุณุงุฑ ฺฉูฺฺฉ ($10^{-7}$ ุง $10^{-8}$) ฺฉู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุชูุณู ุจุฑ ุตูุฑ (ุฒูุงู ฺฉู $G_i$ ุตูุฑ ูโุดูุฏ) ุงุถุงูู ูโุดูุฏ.

**RMSprop (Root Mean Square Propagation)**

* **ูุฏู:** ูุดุงุจู ุขุฏุงฺฏุฑุงุฏุ ุงูุง ูุญูู ุขูพุฏุช $G$ ูุชูุงูุช ุงุณุช ุชุง ุงุฒ ฺฉุงูุด ุณุฑุน ู ุฏุงุฆู ูุฑุฎ ุงุฏฺฏุฑ ุฌููฺฏุฑ ุดูุฏ.
* **ุฑุงุจุทู ุงูุจุงุดุช ฺฏุฑุงุฏุงู ($G_i$):**
    $$G_i = \beta \cdot G_{i-1} + (1-\beta) \cdot \left( \frac{\partial L}{\partial W_i} \right)^2$$
    * ุงุฒ ฺฉ ุถุฑุจ $\beta$ (ูุนูููุงู $0.9$) ุจุฑุง ฺฉุงูุด ุงุซุฑ ฺฏุฑุงุฏุงูโูุง ุฎู ูุฏู ุงุณุชูุงุฏู ูโุดูุฏ.
* **ุฑุงุจุทู ุจูโุฑูุฒุฑุณุงู ูุฒู ($W_{i+1}$):**
    $$W_{i+1} = W_i - \frac{\eta}{\sqrt{G_i + \epsilon}} \cdot \frac{\partial L}{\partial W_i}$$

---

## ต. ุขุฏุงู (Adam - Adaptive Moment Estimation)

**ุถุนู AdaGrad ู RMSprop:** ุงู ุฏู ุงูฺฏูุฑุชู ูฺฺฏ **ููููุชูู** ุฑุง ุฏุงุฎู ุฎูุฏ ูุฏุงุฑูุฏ.

* **ุงุฏู ุขุฏุงู:** ุชุฑฺฉุจ ุจูุชุฑูโูุง RMSprop (ูุฑุฎ ุงุฏฺฏุฑ ุชุทุจู) ู ููููุชูู.
    $$\text{Adam} = \text{RMSprop} + \text{Momentum}$$

* **ุฑุงุจุทู ููุง ุขุฏุงู (ุณูุชุฒ ุดุฏู ุฏุฑ ูุฏู):**
    ุจูโุฌุง ุงุณุชูุงุฏู ุงุฒ ฺฏุฑุงุฏุงู ูุญุธูโุง ุฏุฑ ูุฎุฑุฌ RMSpropุ ุงุฒ ุฑุงุจุทู ููููุชูู ุงุณุชูุงุฏู ูโฺฉูู:
    $$W_{i+1} = W_i - \frac{\eta}{\sqrt{G_i + \epsilon}} \cdot \left( \frac{\partial L}{\partial W_i} + \alpha \cdot \frac{\partial L}{\partial W_{i-1}} \right)$$

* **ูุฒุช:** ุขุฏุงู ุจู ุฏูู ุฏุงุดุชู ูู **ููููุชูู** ู ูู ูุงุจูุช **ุชูุธู ุฏุงูุงูฺฉ ูุฑุฎ ุงุฏฺฏุฑ**ุ ูุณุจุช ุจู ุงุบูุจ ุงูฺฏูุฑุชูโูุง ุฏฺฏุฑุ ุนููฺฉุฑุฏ ุจูููโุชุฑ ู ููุงุณุจโุชุฑ ุฏุงุฑุฏ ู ุจูโุทูุฑ ฺฉู ูุงุจู ูุจููโุชุฑ ุงุณุช.

---
ุงฺฏุฑ ุชูุงู ุฏุงุฑุฏุ ูโุชูุงูู ฺฉ ฺฉุฏ ูพุงุชูู ุณุงุฏู ุจุฑุง ููุงุณู ุจุตุฑ ุญุฑฺฉุช ฺูุฏ ุจูููโุณุงุฒ ูุฎุชูู ุฏุฑ ูุถุง ุฒุงู (Loss Space) ุจุฑุง ุดูุง ูพุฏุง ู ุงุฑุงุฆู ฺฉูู.
